import json
import os
import re
import random
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup

# ---------------------- 1. é…ç½®å‚æ•° ----------------------
TARGET_URL = "https://www.liepin.com/zhaopin/?city=410&dq=410&pubTime=&currentPage=0&pageSize=40&key=%E9%87%91%E8%9E%8D&suggestTag=&workYearCode=1&compId=&compName=&compTag=&industry=&salaryCode=&jobKind=&compScale=&compKind=&compStage=&eduLevel=&otherCity=&sfrom=search_job_pc"
JOB_WEB_DIR = r"D:\PythonCode\PyCrawler\small_project\Comments\JobWeb"
os.makedirs(JOB_WEB_DIR, exist_ok=True)
DATA_SAVE_PATH = os.path.join(JOB_WEB_DIR, "liepin_recruits.json")
MY_WEBSITE_HTML = os.path.join(JOB_WEB_DIR, "liepin_my_website.html")
PAGE_RANGE = range(0, 1)  # æŠ“å–ç¬¬1é¡µ


# ---------------------- 2. è·å–é¡µé¢HTML ----------------------
def get_page_html():
    page_html_list = []
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=False,
                channel="chrome",
                args=["--no-sandbox", "--disable-blink-features=AutomationControlled"]
            )
            page = browser.new_page()

            user_agents = [
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/123.0.0.0 Safari/537.36"
            ]
            page.set_extra_http_headers({"User-Agent": random.choice(user_agents)})

            for page_num in PAGE_RANGE:
                current_url = re.sub(r"currentPage=\d+", f"currentPage={page_num}", TARGET_URL)
                print(f"ğŸ” æŠ“å–ç¬¬{page_num + 1}é¡µï¼š{current_url}")

                page.goto(current_url, wait_until="networkidle")
                page.wait_for_selector('a[data-nick="job-detail-job-info"]', timeout=10000)
                page.wait_for_timeout(random.randint(1000, 2000))

                page_html = page.content()
                page_html_list.append(page_html)
                print(f"âœ… ç¬¬{page_num + 1}é¡µHTMLæŠ“å–å®Œæˆ")

            browser.close()
            return page_html_list
    except Exception as e:
        print(f"âŒ æŠ“å–é¡µé¢å¤±è´¥ï¼š{str(e)}")
        return []


# ---------------------- 3. æ ¸å¿ƒä¿®å¤ï¼šè§£æå‡½æ•°ï¼ˆç¡®ä¿strip()æ˜¯å­—ç¬¦ä¸²æ–¹æ³•ï¼‰ ----------------------
def parse_job_info(page_html_list):
    all_jobs = []
    if not page_html_list:
        return all_jobs

    for page_idx, page_html in enumerate(page_html_list, 1):
        soup = BeautifulSoup(page_html, "lxml")
        job_links = soup.find_all('a', attrs={"data-nick": "job-detail-job-info"})

        if not job_links:
            print(f"âŒ ç¬¬{page_idx}é¡µæœªæ‰¾åˆ°ç›®æ ‡èŒä½æ ‡ç­¾")
            continue

        for job_idx, job_link in enumerate(job_links, 1):
            try:
                # 1. èŒä½è¯¦æƒ…é“¾æ¥ï¼ˆä¿®å¤ï¼šç¡®ä¿hrefæ˜¯å­—ç¬¦ä¸²åè°ƒç”¨strip()ï¼‰
                job_href = job_link.get("href", "").strip()  # æ­£ç¡®ï¼šå­—ç¬¦ä¸²å¯¹è±¡.strip()
                job_url = f"https://www.liepin.com{job_href}" if job_href.startswith("/") else job_href

                # 2. èŒä½åç§°ï¼ˆä¿®å¤ï¼šå…ˆåˆ¤æ–­å…ƒç´ å­˜åœ¨ï¼Œå†å¯¹æ–‡æœ¬è°ƒç”¨strip()ï¼‰
                job_title = "æœªçŸ¥èŒä½"
                job_title_elem = job_link.find("div", class_=re.compile(r"ellipsis-1"))
                if job_title_elem:
                    job_title = job_title_elem.get_text().strip()  # æ­£ç¡®ï¼šæ–‡æœ¬å­—ç¬¦ä¸².strip()

                # 3. è–ªèµ„ï¼ˆä¿®å¤ï¼šåŒä¸Šï¼Œå…ˆåˆ¤æ–­å…ƒç´ ï¼Œå†å¤„ç†æ–‡æœ¬ï¼‰
                salary = "è–ªèµ„é¢è®®"
                salary_elem = job_link.find("span", class_=re.compile(r"job-salary"))
                if salary_elem:
                    salary = salary_elem.get_text().strip()  # æ­£ç¡®ï¼šæ–‡æœ¬å­—ç¬¦ä¸².strip()

                # 4. å·¥ä½œåœ°ç‚¹ï¼ˆä¿®å¤ï¼šå¯¹æ ‡ç­¾æ–‡æœ¬è°ƒç”¨strip()ï¼‰
                location = "æœªçŸ¥åœ°ç‚¹"
                location_box = job_link.find("div", class_=re.compile(r"job-dq-box"))
                if location_box:
                    location_text = location_box.get_text().strip()  # æ­£ç¡®ï¼šæ–‡æœ¬å­—ç¬¦ä¸².strip()
                    location_match = re.search(r"ã€([^ã€‘]+)ã€‘", location_text)
                    if location_match:
                        location = location_match.group(1).strip()  # æ­£ç¡®ï¼šåŒ¹é…ç»“æœå­—ç¬¦ä¸².strip()

                # 5. å²—ä½æ ‡ç­¾ï¼ˆä¿®å¤ï¼šå¯¹æ¯ä¸ªæ ‡ç­¾æ–‡æœ¬è°ƒç”¨strip()ï¼‰
                job_labels = []
                label_elems = job_link.find_all("span", class_=re.compile(r"labels-tag"))
                for label in label_elems:
                    label_text = label.get_text().strip()  # æ­£ç¡®ï¼šæ–‡æœ¬å­—ç¬¦ä¸².strip()
                    if label_text:  # è¿‡æ»¤ç©ºæ ‡ç­¾
                        job_labels.append(label_text)

                # 6. èŒä½æ ‡ç­¾ï¼ˆä¿®å¤ï¼šå¤„ç†æ–‡æœ¬ï¼‰
                urgent_tag = "æ™®é€šèŒä½"
                urgent_tag_elem = job_link.find("span", class_=re.compile(r"job-tag"))
                if urgent_tag_elem:
                    urgent_tag = urgent_tag_elem.get_text().strip()  # æ­£ç¡®ï¼šæ–‡æœ¬å­—ç¬¦ä¸².strip()

                # 7. èŒä½IDï¼ˆä¿®å¤ï¼šå¯¹é“¾æ¥æ–‡æœ¬å¤„ç†ï¼‰
                job_id = f"page{page_idx}_job{job_idx}"
                job_id_match = re.search(r"(\d+)\?", job_href)
                if job_id_match:
                    job_id = job_id_match.group(1).strip()  # æ­£ç¡®ï¼šåŒ¹é…ç»“æœå­—ç¬¦ä¸².strip()

                # æ•´ç†æ•°æ®
                job_info = {
                    "åºå·": len(all_jobs) + 1,
                    "é¡µç ": page_idx,
                    "èŒä½ID": job_id,
                    "èŒä½åç§°": job_title,
                    "è–ªèµ„": salary,
                    "å·¥ä½œåœ°ç‚¹": location,
                    "å²—ä½æ ‡ç­¾": job_labels,
                    "èŒä½æ ‡ç­¾": urgent_tag,
                    "è¯¦æƒ…é“¾æ¥": job_url
                }
                all_jobs.append(job_info)
                print(f"âœ… ç¬¬{page_idx}é¡µç¬¬{job_idx}æ¡ï¼š{job_title}ï¼ˆ{salary} | {location}ï¼‰")

            except Exception as e:
                print(f"âŒ è§£æç¬¬{page_idx}é¡µç¬¬{job_idx}æ¡å¤±è´¥ï¼š{str(e)}")
                continue

    return all_jobs


# ---------------------- 4. ä¿å­˜æ•°æ® ----------------------
def save_data(jobs):
    if not jobs:
        print("âŒ æ— æ•°æ®å¯ä¿å­˜")
        return
    try:
        with open(DATA_SAVE_PATH, "w", encoding="utf-8") as f:
            json.dump(jobs, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°ï¼š{DATA_SAVE_PATH}ï¼ˆå…±{len(jobs)}æ¡ï¼‰")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥ï¼š{str(e)}")


# ---------------------- 5. ç”Ÿæˆå±•ç¤ºç½‘ç«™ ----------------------
def generate_website(jobs):
    if not jobs:
        print("âŒ æ— æ•°æ®ç”Ÿæˆç½‘ç«™")
        return
    html_content = f'''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>çŒè˜é‡‘èè¡Œä¸šèŒä½å±•ç¤º</title>
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; font-family: "Microsoft YaHei", sans-serif; }}
        body {{ background: #f5f7fa; padding: 20px; }}
        .container {{ max-width: 1200px; margin: 0 auto; }}
        .header {{ text-align: center; margin: 30px 0; padding-bottom: 20px; border-bottom: 1px solid #eee; }}
        .header h1 {{ color: #2c3e50; font-size: 28px; }}
        .header p {{ color: #7f8c8d; font-size: 16px; margin-top: 10px; }}
        .job-card {{ background: #fff; border-radius: 10px; padding: 20px; margin-bottom: 20px; box-shadow: 0 2px 10px rgba(0,0,0,0.05); transition: transform 0.3s; }}
        .job-card:hover {{ transform: translateY(-5px); box-shadow: 0 5px 15px rgba(0,0,0,0.1); }}
        .card-top {{ display: flex; justify-content: space-between; align-items: center; margin-bottom: 15px; }}
        .job-title {{ font-size: 20px; color: #e74c3c; font-weight: bold; }}
        .job-salary {{ font-size: 18px; color: #27ae60; font-weight: bold; }}
        .card-mid {{ display: flex; flex-wrap: wrap; gap: 20px; margin-bottom: 15px; }}
        .info-item {{ display: flex; align-items: center; }}
        .info-label {{ color: #7f8c8d; font-size: 14px; width: 80px; }}
        .info-value {{ color: #2c3e50; font-size: 14px; }}
        .tags {{ display: flex; flex-wrap: wrap; gap: 8px; margin-bottom: 15px; }}
        .tag {{ padding: 4px 12px; border-radius: 20px; font-size: 13px; }}
        .label-tag {{ background: #e8f5e9; color: #2e7d32; }}
        .urgent-tag {{ background: #ffebee; color: #c62828; }}
        .detail-btn {{ display: inline-block; background: #3498db; color: #fff; padding: 8px 20px; border-radius: 5px; text-decoration: none; font-size: 14px; transition: background 0.3s; }}
        .detail-btn:hover {{ background: #2980b9; }}
        @media (max-width: 768px) {{
            .card-top {{ flex-direction: column; align-items: flex-start; gap: 10px; }}
            .info-item {{ width: 100%; margin-bottom: 5px; }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>çŒè˜ç½‘ - é‡‘èè¡Œä¸šåº”å±Šç”ŸèŒä½</h1>
            <p>å…±æ”¶å½• {len(jobs)} æ¡æœ‰æ•ˆèŒä½ï¼ˆä¸Šæµ·åœ°åŒºï¼‰</p>
        </div>
'''
    for job in jobs:
        labels_html = "".join([f'<span class="tag label-tag">{tag}</span>' for tag in job["å²—ä½æ ‡ç­¾"]])
        card_html = f'''
        <div class="job-card">
            <div class="card-top">
                <div class="job-title">{job['èŒä½åç§°']}</div>
                <div class="job-salary">{job['è–ªèµ„']}</div>
            </div>
            <div class="card-mid">
                <div class="info-item">
                    <span class="info-label">åœ°ç‚¹ï¼š</span>
                    <span class="info-value">{job['å·¥ä½œåœ°ç‚¹']}</span>
                </div>
                <div class="info-item">
                    <span class="info-label">æ ‡ç­¾ï¼š</span>
                    <span class="info-value">{job['èŒä½æ ‡ç­¾']}</span>
                </div>
                <div class="info-item">
                    <span class="info-label">èŒä½IDï¼š</span>
                    <span class="info-value">{job['èŒä½ID']}</span>
                </div>
            </div>
            <div class="tags">
                {labels_html}
                <span class="tag urgent-tag">{job['èŒä½æ ‡ç­¾']}</span>
            </div>
            <a href="{job['è¯¦æƒ…é“¾æ¥']}" target="_blank" class="detail-btn">æŸ¥çœ‹èŒä½è¯¦æƒ…</a>
        </div>
'''
        html_content += card_html
    html_content += '''
    </div>
</body>
</html>'''
    try:
        with open(MY_WEBSITE_HTML, "w", encoding="utf-8") as f:
            f.write(html_content)
        print(f"âœ… å±•ç¤ºç½‘ç«™å·²ç”Ÿæˆï¼š{MY_WEBSITE_HTML}")
    except Exception as e:
        print(f"âŒ ç”Ÿæˆç½‘ç«™å¤±è´¥ï¼š{str(e)}")


# ---------------------- 6. ä¸»å‡½æ•° ----------------------
def main():
    print("===== å¼€å§‹æŠ“å–çŒè˜ç½‘ç›®æ ‡èŒä½ä¿¡æ¯ =====")
    page_htmls = get_page_html()
    if not page_htmls:
        print("âŒ æœªè·å–åˆ°é¡µé¢æ•°æ®")
        return
    job_list = parse_job_info(page_htmls)
    if not job_list:
        print("âŒ æœªè§£æåˆ°èŒä½ä¿¡æ¯")
        return
    save_data(job_list)
    generate_website(job_list)
    print("===== æŠ“å–æµç¨‹å®Œæˆ =====")


if __name__ == "__main__":
    main()